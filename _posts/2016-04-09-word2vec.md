---
layout: post
type: blog
comments: true
title: Word2Vec CBOW
date: 2016-04-09
categories: machine-learning
---
Vector representations of words learned by word2vec models have proven to carry semantic meaning which are extremely useful for NLP tasks. After reading/watching/googling a lot of papers/lectures/internet on word2vec, I find the notation usage and fancy jargon make it extremely hard to unscramble initial ideas. This blog posts is aimed at helping novice NLP machine learning enthusiasts like me with derivations and basic understanding. Its assumed the reader has some knowledge about neural networks, if not there are wonderful materials on-line to learn about them.

Words in a corpus can be represented as a [document-term matrix](https://en.wikipedia.org/wiki/Document-term_matrix) or concurrence matrix. Lets consider the task of predicting one word (called just "word" or "output word") given a single context-word (called "context" or "output word"). This is the simplest form of Continuous Bag of Words (CBOW) model.  Concurrence matrix - each element represents number of times a word occur beside another word in the windows of certain size, input word is usually called center word (here we consider window size of 1)

|Ever|tried|.|

If we take the center word (input word) as `tried`, then the concurrency vector (matrix with just one row) will be

|     |Ever|tried|.|
|-----|----|-----|-|
|tried|1   |0    |1|

For the following corpus, the document-term matrix is made with case-sensitive words, concurrency matrix has been created with non-case-sensitive words. The examples shows the difference.

|                     Doc 1|                  Doc 2|                    Doc 3|
| ------------------------ | ----------------------| ------------------------|
| Ever tried. <br>Ever failed. | No matter. <br>Try Again. | Fail again. <br>Fail better.|


|Document Term Matrix|![]({{ site.baseurl }}/assets/images/document-term-matrix.png)
|Concurrency Matrix|![]({{ site.baseurl }}/assets/images/concurrency-matrix.png)

Description on the above matrices were to give an idea about how words and dimensions might look like. Lets consider a simple CBOW bigram-model where single target word will be predicted from single context word. Let the size of the vocabulary be V. Then this matrix will be of size VxV. Consider [glove project](http://nlp.stanford.edu/projects/glove/), the size of the vocabulary is huge in terms of millions, the matrix can be gargantuan. So usually the number of dimensions will be reduced using SVD/PCA.

|![]({{ site.baseurl }}/assets/images/input-matrix.png)|

Lets consider the following set up for bigram-model word2vec. Its similar to a simple neural network with one input layer, one hidden layer and one output layer.

|![]({{ site.baseurl }}/assets/images/bigram-w2v.png)|


Each $$x_k$$ and $$y_j$$ is a word in the matrix and we have V words in our vocabulary.

`I represents the index of the input word that is used to predict a word in x vector`

`O represents the index of the output word that needs to be predicted in y vector`


Input word vector is one-hot encoded to represent just one word at a time. In our above example for center word (input word) `tried`, the one hot encoded vector will be

|Ever|tried|.|
|0|1|0|

The input matrix that has V words and D dimensions can be written out as

$$
X =
\left(
    \begin{array}{cccc}
      x_{11} & x_{12} & ... & x_{1i} & ... & x_{1H} \\
      x_{21} & x_{22} & ... & x_{2i} & ... & x_{2H} \\
      : & : & : & : & : & : \\
      x_{k1} & x_{k2} & ... & x_{ki} & ... & x_{kH} \\
      : & : & : & : & : & : \\
      x_{V1} & x_{V2} & ... & x_{Vi} & ... & x_{VH} \\
    \end{array}
\right)

$$

In this setting we have There are $V$ input neural units, $H$ hidden units (same as $D$ dimensions), $V$ output neural units. The dimention of the input data is `D` which will be equal to `H` in our setup. In neural networks usually hidden layer will have a link activation (relu/sigmoid/tanh/...) function, here we consider it to be simply linear. Hidden layer can be represented as

|$h = \mathbf{X}^\top * W$|

Lets say `k`th word in the input vector is the context/center/input word ( the word that has 1 in the one-hot vector encoding representation),

$$
x_{qr} = 0
\begin{cases}
q !=k,   \text range (0,V)\\
r, \text range (0,D)
\end{cases}
$$

then h becomes,

$$
h =
\left(
    \begin{array}{cccc}
      x_{11} & x_{21} & ... & x_{k1} & ... & x_{V1} \\
      x_{12} & x_{22} & ... & x_{k2} & ... & x_{V2} \\
      : & : & : & : & : & : \\
      x_{1i} & x_{2i} & ... & x_{ki} & ... & x_{Vi} \\
      : & : & : & : & : & : \\
      x_{1H} & x_{2H} & ... & x_{kH} & ... & x_{VH} \\
    \end{array}
\right)
*
\left(
    \begin{array}{cccc}
      w_{11} & w_{12} & ... & w_{1i} & ... & w_{1H} \\
      w_{21} & w_{22} & ... & w_{2i} & ... & w_{2H} \\
      : & : & : & : & : & : \\
      w_{k1} & w_{k2} & ... & w_{ki} & ... & w_{kH} \\
      : & : & : & : & : & : \\
      w_{V1} & w_{V2} & ... & w_{Vi} & ... & w_{VH} \\
    \end{array}
\right)
\\
=
\left(
    \begin{array}{ccc}
      0 & 0 & ... & 0 & ... & 0 \\
      0 & 0 & ... & 0 & ... & 0 \\
      : & : & ... & : & ... & 0 \\
      x_{k1}*w_{k1} & x_{k2}*w_{k2} & ... & x_{ki}*w_{ki} & ... & x_{kD}*w_{kH} \\
      : & : & ... & : & ... & 0 \\
      0 & 0 & ... & 0 & ... & 0 \\
    \end{array}
\right)
$$

Now , since the input vector is one-hot encoded, we see all the matrix multiplication will end up being zero, except for `k` row in the `h` matrix. Also we know $$x_k = 1$$ which makes it `h` matrix contain just one non zero row  which is the vector representation of center/input word. Since `h` is very sparse we can call it

$$h = v_{wI}  \tag{h_for_bigram} \label{h_for_bigram}$$

Now consider the next intersection, hidden layer to output layer. The weight matrix is represented as $$W'$$.

|$u = \mathbf{W'}^\top * v_{wI}$|
|$u_j = \mathbf{W_{y_{col_j}}}^\top * v_{wI}$|
|$$u_j = \mathbf{v'_{w_j}}^\top * v_{wI}$$|

$$
u =
\mathbf{
\left(
    \begin{array}{cccc}
      w'_{11} & w'_{12} & ... & w'_{1V} \\
      w'_{21} & w'_{22} & ... & w'_{2V} \\
      : & : & ... & : \\
      w'_{i1} & w'_{ij} & ... & w'_{iV} \\
      : & : & ... & : \\
      w'_{H1} & w'_{H2} & ... & w'_{HV}
    \end{array}
\right)
}^\top
*
\left(
    \begin{array}{c}
      v_{wI_1} \\
      v_{wI_2} \\
      : \\
      v_{wI_i} \\
      : \\
      v_{wI_V} \\
    \end{array}
\right)
 =
\left(
    \begin{array}{cccccc}
      w'_{11} & w'_{21} & ... & w'_{i1} & ... & w'_{V1} \\
      w'_{12} & w'_{22} & ... & w'_{ij} & ... & w'_{V2} \\
      : & : & ... & : & ... & : \\
      w'_{1V} & w'_{2V} & ... &  w'_{iV} & ... & w'_{VH}
    \end{array}
\right)
*
\left(
    \begin{array}{c}
      v_{wI_1} \\
      v_{wI_2} \\
      : \\
      v_{wI_i} \\
      : \\
      v_{wI_V} \\
    \end{array}
\right)
\\ =
\left(
    \begin{array}{c}
      v_{wI_1} * w'_{11} + v_{wI_2} * w'_{21} + ... + v_{wI_i} * w'_{i1} + ... + v_{wI_H} * w'_{H1} \\
      v_{wI_1} * w'_{12} + v_{wI_2} * w'_{22} + ... + v_{wI_i} * w'_{i2} + ... + v_{wI_H} * w'_{H2} \\
      : \\
      v_{wI_1} * w'_{1j} + v_{wI_2} * w'_{2j} + ... + v_{wI_i} * w'_{ij} + ... + v_{wI_H} * w'_{Hj} \\
      : \\
      v_{wI_1} * w'_{1V} + v_{wI_2} * w'_{2V} + ... + v_{wI_i} * w'_{iV} + ... v_{wI_H} * w'_{HV}
    \end{array}
\right)
 =
\left(
    \begin{array}{c}
      u_1 \\
      u_2 \\
      : \\
      u_o \\
      : \\
      u_j \\
      : \\
      u_V
    \end{array}
\right)
=
\left(
    \begin{array}{c}
      \mathbf{v'_{w_1}}^\top . v_{wI} \\
      \mathbf{v'_{w_2}}^\top . v_{wI} \\
      : \\
      \mathbf{v'_{wO}}^\top . v_{wI} \\
      : \\
      \mathbf{v'_{w_j}}^\top . v_{wI} \\
      : \\
      \mathbf{v'_{w_V}}^\top . v_{wI}
    \end{array}
\right)
=
\left(
    \begin{array}{c}
      \mathbf{W_{y_{col_1}}}^\top . v_{wI} \\
      \mathbf{W_{y_{col_2}}}^\top . v_{wI} \\
      : \\
      \mathbf{W_{y_{col_o}}}^\top . v_{wI} \\
      : \\
      \mathbf{W_{y_{col_j}}}^\top . v_{wI} \\
      : \\
      \mathbf{W_{y_{col_V}}}^\top . v_{wI}
    \end{array}
\right)
$$

notation used in papers $$\mathbf{v'_{W'_j}}^\top$$, represents same as $$\mathbf{W_{y_{col_j}}}^\top$$

Then there is a [soft-max](http://cs231n.github.io/linear-classify/#softmax) layer, a log-linear classification model to obtain posterior distribution of the words which is multinomial distribution.

$$y_i = P(word_j|word_I)= \frac{exp(u_j)}{\sum_{j=0}^V exp(u_j)}$$

The required word/unit that needs to be predicted turns out to be

$$y_o = \frac{exp(\mathbf{v'_{wO}}^\top . v_{wI})}{\sum_{j=0}^V exp(\mathbf{v'_{w_j}}^\top . v_{wI})}$$

If we think about it, both the vectors are 2 different representation of the same word.

|$$v_{wI}$$|$$v'_{wO}$$|

We have to find the parameters ($W$ and $W'$) such that given $v_{wI}$ we get maximum likelihood for $v'_{wO}$, which can be written as follows

$$ L = {P(v'_{wO}|v_{wI})} $$

We take log likelihood, for mathematical ease,

$$ \log L = {\log P(v'_{wO}|v_{wI})} $$

$$ = { \log \frac{exp(\mathbf{v'_{wO}}^\top . v_{wI})}{\sum_{j=0}^V exp(\mathbf{v'_{w_j}}^\top . v_{wI})}}
\\
 = { \mathbf{v'_{wO}}^\top . v_{wI}} - { \log \sum_{j=0}^V exp(\mathbf{v'_{w_j}}^\top . v_{wI})}
$$

Lets consider the following Loss Function for which we have to find the minimum. We can also think of this as Cross Entropy function between the predicted output and the expected output. Both yields the same following equation.

$$
E = - \log L
 = { \log \sum_{j=0}^V exp(\mathbf{v'_{w_j}}^\top . v_{wI})} - { \mathbf{v'_{wO}}^\top . v_{wI}}
$$

We have the feed-forward network with objective function and of-course we can use back-propagation to train the network, lets derive gradients for the network using the objective function.

derivative w.r.t matrix $W'$ or each element $$w'_{ij}$$ or preferably vector $$v'_{wj}$$

$$
\frac{\partial E}{\partial v'_{wj}} = { \frac{exp(\mathbf{v'_{wj}}^\top . v_{wI})}{\sum_{j=0}^V exp(\mathbf{v'_{w_j}}^\top . v_{wI})} * v_{wI} } - { t_i * v_{wI} }
\\
= v_{wI} * ( { \frac{exp(\mathbf{v'_{wj}}^\top). v_{wI}}{\sum_{j=0}^V exp(\mathbf{v'_{w_j}}^\top . v_{wI})} } - { t_i } )
\\
= v_{wI} * ( y_j - t_j)
\\
\begin{cases}
t_j=1,  & \text{if j = O, matched expected output word} \\
t_j=0, & \text{otherwise}
\end{cases}

$$

derivative w.r.t matrix $W$ or each element $$w_{ij}$$ or preferably vector $$v_{wI}$$

$$
\frac{\partial E}{\partial v_{wI}} =  \sum_{j=0}^V \frac{exp(\mathbf{v'_{wj}}^\top . v_{wI})}{\sum_{j=0}^V exp(\mathbf{v'_{w_j}}^\top . v_{wI})} * v'_{wj} -  v'_{wO}
\\
= \sum_{j=0}^V  y_j * v'_{wj} - v'_{wO}
\\
= \sum_{j=0}^V  v'_{wj} (y_j *  - t_j)
\\
\begin{cases}
t_j=1,  & \text{if j = O, matched expected output word} \\
t_j=0, & \text{otherwise}
\end{cases}
$$

Hence the update equations can be written as with learning rate $$ \eta $$

$$
{v'_{wj}}^{new} = {v'_{wj}}^{old} - \eta . [v_{wI} * ( y_j - t_j)]
\\
{v_{wI}}^{new} = {v_{wI}}^{old} - \eta . \sum_{j=0}^V  v'_{wj} (y_j *  - t_j)
$$

Reason we have just $$v'_{wj}$$ and $$v_{wI}$$, because these vectors contain all the $$w_{ki}$$ and $$w'_{ij}$$ values in them which comprises of our learn-able parameters $W$ and $W'$

The bigram-model helped us predict a single target/central/output word given a single context/input word, if we want to predict a single target/central/output word given C number of context/input words, then h from above \eqref{h_for_bigram} becomes

$$h = {\frac{1}{C}} * ({v_{w1} + v_{w2} + v_{wC} + ... + v_{wN}}) $$

|![]({{ site.baseurl }}/assets/images/ngram-w2v.png)|

After similar derivation, the update equations become

$$
{v'_{wj}}^{new} = {v'_{wj}}^{old} - \eta . [v_{wI} * ( y_j - t_j)]
\\
{v_{wI}}^{new} = {v_{wI}}^{old} - {\frac{1}{C}} . \eta . \sum_{j=0}^V  v'_{wj} (y_j *  - t_j)
$$

Now if one understands the above derivation of update rules, then comprehending the evolved concepts of skipgram and negative sampling can be little easier. Check references for papers explaining derivations on skipgram and negative sampling methods.

Let me know if there are mistakes :)

References:

* [From Frequency to Meaning: Vector Space Models of Semantics](http://arxiv.org/abs/1003.1141)
* [word2vec Parameter Learning Explained](http://arxiv.org/abs/1411.2738)
* [CS224d 2016 Lectures](http://cs224d.stanford.edu/syllabus.html)
* [word2vec Explained: Deriving Mikolov et al.'s Negative-Sampling Word-Embedding Method](http://arxiv.org/abs/1402.3722)
* [Univ of Waterloo word2vec lectures](https://www.youtube.com/watch?v=TsEGsdVJjuA)

